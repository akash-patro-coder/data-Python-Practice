{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d85f78b6-665d-4c32-9d12-a751fe7cc8b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data - Engineering Assessment using python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb02bef9-26f4-4e84-9635-a39d8041d83e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# === Configure ===\n",
    "FILE_PATH = \"/Volumes/workspace/customers/data/data.csv\"   # Your given path\n",
    "\n",
    "# Logging config: visible in Databricks cell output\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(\"customer_ingestion\")\n",
    "\n",
    "logger.info(\"Notebook started\")\n",
    "\n",
    "# Optional sanity checks — helpful when debugging path issues in Databricks\n",
    "try:\n",
    "    exists_local = os.path.exists(FILE_PATH)  # checks driver filesystem\n",
    "    logger.info(f\"Local driver path exists? {exists_local}\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Local path check failed: {e}\")\n",
    "\n",
    "# Databricks Filesystem (DBFS) listing for the Volume directory\n",
    "# (This uses the DBFS API path style; it will work if UC Volumes are enabled.)\n",
    "try:\n",
    "    _ = dbutils.fs.ls(\"dbfs:/Volumes/workspace/customers/data\")\n",
    "    logger.info(\"DBFS listing successful for dbfs:/Volumes/workspace/customers/data\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"DBFS path check failed (this can be normal if UC is not enabled): {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9beb9841-0d49-476f-91b5-ec360ab4fda1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "REQUIRED_COLUMNS = [\"Customer Id\", \"First Name\", \"Email\"]  # minimal required fields\n",
    "OPTIONAL_COLUMNS = [\n",
    "    \"Last Name\", \"Company\", \"City\", \"Country\", \"Phone 1\", \"Phone 2\", \"Subscription Date\"\n",
    "]\n",
    "\n",
    "def _to_bool(value):\n",
    "    \"\"\"Robust boolean parser for typical CSV values.\"\"\"\n",
    "    if value is None:\n",
    "        return None\n",
    "    v = str(value).strip().lower()\n",
    "    if v in (\"true\", \"t\", \"yes\", \"y\", \"1\"):\n",
    "        return True\n",
    "    if v in (\"false\", \"f\", \"no\", \"n\", \"0\"):\n",
    "        return False\n",
    "    return None  # Unknown → leave as None\n",
    "\n",
    "def validate_and_transform(row: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Business validation + normalization.\n",
    "    - Ensures required fields exist and are non-empty\n",
    "    - Validates Customer Id is an integer\n",
    "    - Minimal email sanity check\n",
    "    - Coerces Subscription Date to bool when present\n",
    "    Raises:\n",
    "        ValueError: if the row violates business rules (ingestion-safe — caught by caller)\n",
    "    \"\"\"\n",
    "    # 1) Required presence\n",
    "    for col in REQUIRED_COLUMNS:\n",
    "        if col not in row or str(row[col]).strip() == \"\":\n",
    "            raise ValueError(f\"Missing required '{col}'\")\n",
    "\n",
    "    # 2) Type + format checks\n",
    "    try:\n",
    "        row[\"Customer Id\"] = int(str(row[\"Customer Id\"]).strip())\n",
    "    except Exception:\n",
    "        raise ValueError(f\"Invalid Customer Id: {row.get('Customer Id')}\")\n",
    "\n",
    "    email = str(row[\"Email\"]).strip()\n",
    "    if \"@\" not in email or \".\" not in email.split(\"@\")[-1]:\n",
    "        raise ValueError(f\"Invalid Email: {email}\")\n",
    "    row[\"Email\"] = email\n",
    "\n",
    "    # 3) Optional coercions\n",
    "    if \"Subscription Date\" in row:\n",
    "        parsed = _to_bool(row[\"Subscription Date\"])\n",
    "        # Keep None if unknown, otherwise store True/False\n",
    "        row[\"Subscription Date\"] = parsed if parsed is not None else row[\"Subscription Date\"]\n",
    "\n",
    "    # Trim simple text fields\n",
    "    for k in (\"First Name\", \"Last Name\", \"Company\", \"City\", \"Country\", \"Phone 1\", \"Phone 2\"):\n",
    "        if k in row and row[k] is not None:\n",
    "            row[k] = str(row[k]).strip()\n",
    "\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a842986a-6716-4a28-b6c2-e668817fd583",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "REQUIRED_COLUMNS = [\"Index\", \"Customer Id\", \"First Name\", \"Last Name\", \"Email\"]\n",
    "\n",
    "def validate_and_transform(row: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Validates and normalizes a customer record row against business rules.\n",
    "    \"\"\"\n",
    "    # 1) Required presence\n",
    "    for col in REQUIRED_COLUMNS:\n",
    "        if col not in row or str(row[col]).strip() == \"\":\n",
    "            raise ValueError(f\"Missing required '{col}'\")\n",
    "\n",
    "    # 2) Index must be integer\n",
    "    try:\n",
    "        row[\"Index\"] = int(row[\"Index\"])\n",
    "    except Exception:\n",
    "        raise ValueError(f\"Invalid Index value: {row.get('Index')}\")\n",
    "\n",
    "    # 3) Customer Id → keep as string (no int conversion)\n",
    "    row[\"Customer Id\"] = str(row[\"Customer Id\"]).strip()\n",
    "\n",
    "    # 4) Email validation\n",
    "    email = str(row[\"Email\"]).strip()\n",
    "    if \"@\" not in email or \".\" not in email.split(\"@\")[-1]:\n",
    "        raise ValueError(f\"Invalid Email: {email}\")\n",
    "    row[\"Email\"] = email\n",
    "\n",
    "    # 5) Subscription Date → parse as date if not empty\n",
    "    if row.get(\"Subscription Date\", \"\").strip():\n",
    "        try:\n",
    "            row[\"Subscription Date\"] = datetime.strptime(\n",
    "                row[\"Subscription Date\"], \"%Y-%m-%d\"\n",
    "            ).date()\n",
    "        except Exception:\n",
    "            raise ValueError(f\"Invalid Subscription Date: {row['Subscription Date']}\")\n",
    "\n",
    "    # 6) Website validation (basic URL check)\n",
    "    if row.get(\"Website\", \"\").strip():\n",
    "        if not re.match(r\"^https?://\", row[\"Website\"].strip()):\n",
    "            raise ValueError(f\"Invalid Website URL: {row['Website']}\")\n",
    "\n",
    "    # 7) Clean up other string fields\n",
    "    for k, v in row.items():\n",
    "        if isinstance(v, str):\n",
    "            row[k] = v.strip()\n",
    "\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0a84b99-889c-4ad4-8fd6-684cc1ef9082",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "new_customer = {\n",
    "    \"Index\": 999,\n",
    "    \"Customer Id\": \"CUST999\",\n",
    "    \"First Name\": \"Akash\",\n",
    "    \"Last Name\": \"Patro\",\n",
    "    \"Company\": \"Self\",\n",
    "    \"City\": \"Bhubaneswar\",\n",
    "    \"Country\": \"India\",\n",
    "    \"Phone 1\": \"\",\n",
    "    \"Phone 2\": \"\",\n",
    "    \"Email\": \"akash@example.com\",\n",
    "    \"Subscription Date\": \"2025-09-04\",\n",
    "    \"Website\": \"https://akashpatro.com\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Step 1: Read existing data\n",
    "    with open(FILE_PATH, mode=\"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        fieldnames = reader.fieldnames\n",
    "        if not fieldnames:\n",
    "            raise ValueError(\"CSV has no header row\")\n",
    "        rows = list(reader)  # load all rows\n",
    "\n",
    "    # Step 2: Add the new record (ensuring schema alignment)\n",
    "    row_out = {col: new_customer.get(col, \"\") for col in fieldnames}\n",
    "    rows.append(row_out)\n",
    "\n",
    "    # Step 3: Write everything back (overwrite mode)\n",
    "    tmp_file = tempfile.NamedTemporaryFile(delete=False, mode=\"w\", newline=\"\", encoding=\"utf-8\")\n",
    "    try:\n",
    "        writer = csv.DictWriter(tmp_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "    finally:\n",
    "        tmp_file.close()\n",
    "\n",
    "    # Step 4: Replace original file\n",
    "    shutil.move(tmp_file.name, FILE_PATH)\n",
    "    logger.info(f\"✅ Successfully rewrote file with new record: {row_out}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    logger.error(f\"File not found: {FILE_PATH}\")\n",
    "except Exception as e:\n",
    "    logger.exception(f\"❌ Failed to update file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3d1c864-f89a-4e66-b4ea-115b9ef890cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "post_good = 0\n",
    "post_bad = 0\n",
    "\n",
    "try:\n",
    "    with open(FILE_PATH, mode=\"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        # Header sanity\n",
    "        if reader.fieldnames is None or any(col not in reader.fieldnames for col in REQUIRED_COLUMNS):\n",
    "            missing = [c for c in REQUIRED_COLUMNS if not reader.fieldnames or c not in reader.fieldnames]\n",
    "            raise ValueError(f\"CSV header missing required columns: {missing}\")\n",
    "\n",
    "        for i, row in enumerate(reader, start=2):\n",
    "            try:\n",
    "                _ = validate_and_transform(row)\n",
    "                post_good += 1\n",
    "            except ValueError:\n",
    "                post_bad += 1\n",
    "\n",
    "    logger.info(f\"Post-append recheck → valid: {post_good} | invalid: {post_bad}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    logger.error(f\"File not found on recheck: {FILE_PATH}\")\n",
    "except ValueError as ve:\n",
    "    logger.error(f\"ValueError on recheck: {ve}\")\n",
    "except Exception as e:\n",
    "    logger.exception(f\"Unexpected error on recheck: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DE-Python assessment",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
